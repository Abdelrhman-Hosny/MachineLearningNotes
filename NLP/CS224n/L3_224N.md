# **<u>CS224N L3</u>**

## **<u>Cross Entropy loss / error</u>**

A topic from information theory.

If we have a probability distribution $p$ and a computed model probability $q$.				 The cross entropy is:
$$
H(p,q) = - \sum_{c=1}^Cp(c)\,log\,q(c) \\\,\\J(\theta) = \sum_{i=1}^MH(y,\tilde{y})=- \sum_{i=1}^M\sum_{c=1}^Cy\,log\,\tilde{y}
$$
Cross entropy is also used Semi-Supervised Learning , since the labels are guessed then we label them as follows $[ 0.3 , 0.2 , 0.5 , 0]$ , as we are not sure what the correct label is , then we would use cross entropy for a whole distribution instead of using it on one-hot vectors.

------

## **<u>Classification difference with word vectors</u>**

Common in NLP , deep learning:

- We learn both **W** and **word vectors** x
- We learn both **conventional parameters** $W$ and **word representations** 
- The word vectors re-represent one-hot vectors -- move them around in an intermediate layer vector space-- for easy classification with a (linear) softmax classifier via layer x = Le ( it is like adding an extra layer L before your neural network which gets multiplied by a one-hot vector to get a word )

N.B. since word are one hot vectors, you don't need to do matrix multiplication instead you select the column/row that has the vector corresponding to that word.
$$
\nabla_\theta=\left[\begin{array}\,\nabla_{W1}\\\vdots\\\nabla_{Wd}\\\nabla_{x_{\text{aardvark}}}\\\vdots\\\nabla_{x_{\text{zebra}}}\end{array}\right] \epsilon \,\,\R^{Cd+Vd}
$$
Where $Vd$ is a very large number of parameters.

------

## **<u>Named Entity Recognition (NER)</u>**

The task to **find** and **classify** names in text.

If we consider the sentence "Museums in Paris are amazing" , in which we want to classify whether or not the center word "Paris" is a named-entity. In such cases , we wouldn't only want to capture the presence of certain words , but we would want to capture the interactions that happen between words. For instance , maybe it should matter that "Museums" is the first word only if "in" is the second word.

Such non-linear decisions  can often be captured. for that we need a more complex model than a simple softmax.
$$
s = U^Ta = U^Tf(Wx+b)
$$
where $f$ is the activation function.

**Analysis of Dimensions**: If we represent each word using a 4-dimensional word vector and we use a 5-word window as input, the input  $ x \, \epsilon \, \, \R$. If we use 8 sigmoid units in the hidden layer and generate 1 score output from the activations, then $ W \epsilon \,\,\R^{8\text{x}20} , b \,\, \epsilon \,\R^{8} , \, U \epsilon \,\R^{8\text{x}1} , s\, \epsilon\,\R$

### **<u>Maximum Margin Objective Function</u>**

The idea behind this metric is to ensure that the score of the "true" labeled data points is higher than the score computed for "false" labeled data points.

â€‹	Using the previous example , "Museums in Paris are amazing" which is labeled true and has a score $s$ and another "Not all museums in Paris" as a false label with score $s_c$ (c for corrupt) (objective is to predict whether the center word is a named-entity).

Our objective function would be to maximize $(s - s_c)$ or minimize $(s_c - s)$ . But our model is at fault only if $s_c > s$ , we consider the error to be zero if $s > s_c$. Thus , our optimization objective is now:
$$
minimize\,J=max(s_c-s,0)
$$
However , this is risky as it doesn't create a margin of safety , so we change the function to :
$$
minimize \, J = max(\Delta+s_c-s,0)
$$
This delta acts as a safety margin to make sure our model performs even better.

we can scale this margin such that $\Delta = 1$ and let the other parameters adapt to this without any change in performance. ( This is covered in the study of SVMs - geometrical and functional margins).

Finally, we define the following optimization objective which we optimize over all training windows:
$$
minimize \,\,J=max(1+s_c-s,0)
$$
where $s_c=U^Tf(Wx_c+b)$ and $s=U^Tf(Wx+b)$

------

