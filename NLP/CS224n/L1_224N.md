 

# <u>CS224N NLP:</u>

## Lecture 1:

### <u>How do we have usable meaning in a computer ?</u>

WordNet is used which contains lists of **synonyms** & **hypernyms** but it doesn't work that well because even though two words may have the same meaning , Only in some contexts.

And it misses the new meanings of words (e.g badass , nifty , ninja ... etc)

It requires human labor to create and adapt.

Can't measure similarity.

#### <u>Word representation in nlp</u>

### <u>One hot vectors</u> 

Classis nlp : the era before 2013 before neural networks came into play

at the era of Classic nlp , people used words as discrete symbols (through one hot vectors).

similar to WordNet , there are no similarities between one hot vectors.

###  <u>Distributional Semantics</u>

<u>Distributional Semantics</u> : A word's meaning is given y the words that frequently appear close-by.

In nlp, the context of a word is the set of words that appear nearby (within a fixed-size window).

### <u>Word Vectors (embeddings)</u>

We build dense vectors for each word depending on its context.

If two word vectors are similar , their dot product is closer to zero.

------

#### <u>**Word2Vec Overview**</u>

### **Main Idea:**

1. Have a large corpus of text
2. Every word in a fixed vocabulary is represented by a **vector**
3. Go through each position t in the text , which has a center word c and context words o
4. Use the **similarity of the word vectors** for c and o to **calculate the probability of ** <u>o given c (or vice versa)</u>
5. Keep adjusting the word vectors to **maximize** this probability.

### **Objective Function**:

$$
Likelihood = L(\theta) = \prod\limits_{t=1}^T\prod\limits_{-m\le j\le m
 \\  j \ne0} P(w_{t+j}|w_t;\theta)
$$

Where m is the size of the window , T the number of words in your dictionary.
$$
J(\theta) = - \frac1T log L(\theta)= - \frac{1}{T} \sum_{t=1}^T\sum_{-m\le j\le m \\ \text{    } j \ne 0} log(P(w_{t+j}|w_t;\theta))
$$
Minimizing the cost function is the same as maximizing the likelihood.

### Calculating  $P(w_{t+j}|w_t;\theta)$ :

To do that we use two vectors per word w:

- $v_w$ when w is center word
- $u_w$ when w is a context word

Then for a center word c and a context word o :
$$
P(o|c) = \frac{\exp{u_o^Tv_c}}{\sum_{w\epsilon V}\exp{u_w^Tv_c}}\\\,\\
log\,P(o|c) = u_o^Tv_c - log(\sum_{w\epsilon V}\exp{u_w^Tv_c})
$$
$P(u_{\text{problems}} | v_\text{into})$ is short for $P(\text{problems}|\text{into};u_{\text{problems}} ,v_\text{into},\theta)$

Which translates to : probability of the word problem given the word into as parametrized by center word into and context word problems and theta.

### Training the model:

We computer all vector gradients.

Recall that our only parameters in this model are the word vectors , and that for each vector there are two representations v as center word and u as context word.
$$
\theta = \left[\begin{matrix} v_\text{aardvark}\\v_\text{a}\\\vdots\\v_\text{zebra}\\u_\text{aardvark}\\u_\text{a}\\\vdots\\u_\text{zebra} \end{matrix}\right]
$$
We optimize these parameters by using gradient descent.

#### **Training Steps:**

1. Initialize all vectors randomly
2. pass through the corpus
3. calculate the gradients that maximize the likelihood of context words occurring in context of the center word.
4. Perform gradient descent using gradients from step 3
5. repeat the process until it converges

#### **Gradient Formulas**:

$$
\frac{\partial}{\partial v_c} log(o|c) = u_o - \sum_{x=1}^V \frac{\exp{(u_x^Tv_c)}}{\sum_{w=1}^V\exp{(u_w^Tv_c)}}u_x\\=u_o - \sum_{x=1}^V P(x|c)u_x
$$

$\sum_{x=1}^V P(x|c)u_x$ is the expectation value of the vector according to our model.

This is subtracted from the current context word which results in the direction of steepest descent.

