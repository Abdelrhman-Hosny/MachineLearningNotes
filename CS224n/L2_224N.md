# <u>**CS224n Lecture 2**</u>

### <u>**Review : Word2vec**</u>

- Iterate through the whole corpus
- Predict surrounding words using word vectors
- minimize the cost function to maximize the likelihood of words close to the center word

N.B. Word vectors in DL frameworks are represented as rows.

#### **<u>Stop words in Word2vec</u>**

 Stop words occur very frequently which results in them having a high similarity with nearly all other vectors.

This isn't specific to word2vec but also in other word embeddings.

A way that this is dealt with is that by decomposing , the direction with the highest value is usually the one with stopwords . so if you remove that you should be fine.

#### **<u>Two dimensional representation of word vectors</u>**

When elements are reduced to the second dimension they lose some information , so the 2D representation isn't completely accurate.

One of the features of high dimensional spaces is that an element can be close to other words in different direction ( other words not close to each other )

#### <u>**Optimization**</u>

From the last lecture we know that the word2vec uses gradient descent to minimize the objective function and that the objective functions runs through the whole corpus for each iteration.

This is the same problem that happens with batch gradient descent as corpora are around millions to billions of words. So we use Mini-Batch GD.

##### **<u>Stochastic GD with word vectors</u>**

We consider as if a single window is our whole corpus and take steps accordingly.

##### <u>**Mini-Batch GD with word vectors**</u>

However for optimization and for getting less noisy estimates , we use a $2^x$ number of windows (around $2^5 = 32 \text{ or }2^6 = 64$ windows per batch and consider those the whole corpus and perform word2vec accordingly.

##### **<u>Weight update in Stochastic and Mino-Batch GD</u>**

In both algorithms , we only use a subset of words out of all the vocabulary we have , so when we update the weights , if we don't find a smart way to do so , we would end up with a very sparse matrix and a lot of memory waste.

Solution : Find a way to update only the rows of the words in the "mini-corpus" or keep a hash for word vectors

If you are doing distributed computing ( training on different computers ). It's even more important to figure this out as you'd be sending a huge amount of data otherwise.

#### <u>More Details about word2vec</u>

###### <u>Why use two vectors ?</u>

Easier optimization. Both are averaged at the end to obtain the final result.

- Can be done with just one vector per word

###### **<u>Two model variants:</u>**

1. Skip-Gram(SG):

   ​	Predict the context("outside") words (position independent) given center word

2. Continuous Bag of Words (CBOW)

   ​	Predict center word from (bag of) context words

   We presented the **Skip-gram model**

### <u>Negative Sampling</u>

$$
J = log\, \sigma(v^{'\,\,\,\,\,T}_{wo}v_{w1}) + \sum^k_{i=1}E_{w_i}∼ P_n(w) [log\,\sigma( - v^{'\,\,\,\,\,T}_{wi}v_{w1})]
$$

The objective is to maximize this function. we do this by

in Layman's term :

The difference between using softmax or a variation of it and negative sampling is that negative sampling takes a small sample that consists of 1 word that is actually in the context $v_{w1}$ and then k words that are not in the context (mostly) . While in normal softmax it maximizes the word and context while minimizing all the other words in the corpus.

### <u>Co-occurrence  matrix  to get word vectors</u>

With a co-occurrence matrix $X$ 

- 2 options : windows vs full document
- Window: Similar to word2vec , it captures both syntactic(POS) and semantic information
- Document Co-occurrence matrix : will give general topics 9all sports terms will have similar entries) leading to *Latent Semantic Analysis*

#### <u>**Problems with simple co-occurrence vectors**</u>

- Increase in size with the vocabulary
- Very high dimensional (storage problems)
- Subsequent classification models have sparsity issues (leads to less robust models)
- Quadratic cost to train(i.e Performing SVD)

##### **<u>Solutions to the problem</u>**

###### <u>**Method 1 : Dimensionality reduction on X**</u>

Apply SVD or other dimensionality reduction to reduce the number of columns without losing a lot of data

###### **<u>Methods by Rohde et al. 2005</u>**

There are a few method used that improved the algorithm.

1. Problem: function words (the , he , has) are too frequent -> syntax has too much impact

   Fixes that helped *a lot* :

   - Take the log scale of counts
   - take $min(X,t)$ where $t \approx 100$ 
   - Ignore these words
   - Use Pearson correlations instead of counts and set negative values to zero

- Ramped windows that gives words closer to the center word a higher count (weight).

**N.B.** A lot of these "hacks" are also used in word2vec

### **<u>Count based vs direct prediction</u>**

| Count                                                        | Direct prediction                                            |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| LSA , HAL(Lund & Burgess) - COALS,Hellinger-PCA(Rhode et al,Lebret & Collobert) | Skip-gram/CBOW(Mikilov et al.) - NNLM,HLBL,RNN (Bengio et al ; Collobert & Weston) |
| Fast Training                                                | Scales with corpus size                                      |
| Efficient usage of statistics                                | Inefficient usage of statistics                              |
| Primarily used to capture word similarity                    | Generate improved performance on other tasks                 |
| Disproportionate importance given to large counts            | Can capture complex patterns beyond word similarity          |

------

### <u>**Encoding meaning in vector  differences**</u>

**Crucial insight**: Ratios of co-occurrence probabilities can encode meaning components

|                                  | $x$ = solid | $x$ = gas | $x$ = water  | $x$ = random |
| -------------------------------- | ----------- | --------- | ------------ | ------------ |
| $P(x|ice)$                       | **large**   | small     | **large**    | small        |
| $P(x|steam)$                     | small       | **large** | **large**    | small        |
| $\frac{P(x|ice)}{P(x|steam)}$ |**large** | **large**   | small     | $\approx 1 $ |

Actual values from the table above.

|                                  | $x$ = solid   | $x$ = gas     | $x$ = water   | $x$ = random  |
| -------------------------------- | ------------- | ------------- | ------------- | ------------- |
| $P(x|ice)$                       | $1.9*10^{-4}$ | $6.6*10^{-6}$ | $3.0*10^{-3}$ | $1.7*10^{-5}$ |
| $P(x|steam)$                     | $2.2*10^{-5}$ | $7.8*10^{-4}$ | $2.2*10^{-3}$ | $1.8*10^{-5}$ |
| $\frac{P(x|ice)}{P(x|steam)}$ |8.6 | 8.9           | $8.5*10^{-2}$ | 1.36          |



#### **<u>How to capture rations of co-occurrence probabilities as linear meaning components in a word vector space ?</u>**

A: Log-bilinear model :
$$
w_i.w_j = log\,P(i|j)
\\
\text{with vector differences}\,\,\,\,w_x.(w_a - w_b) = log\,\frac{P(x|a)}{P(x|b)}\\\,\\\,\\J = \sum_{i,j = 1}^V f(X_{ij})(w_i^Tw_j+b_i+b_j-log\,X_{ij})^2
$$

- Fast training
- Scalable to huge corpora
- Good performance even with small corpus and small vectors

$f(X_{ij})$ is the co-occurrence count between word i and j . This function is also clipped at a certain value to reduce the effect of very common occurring words.

This is called the GloVe method. The idea behind this method was to use both methods of count based and prediction based algorithm to make word vectors.

------

### <u>**Evaluating word vectors**</u>

#### <u>**Extrinsic vs Intrinsic**</u>

Generally in nlp there are two types of evaluation Extrinsic and Intrinsic

##### **<u>Intrinsic</u>**

- Evaluation on a specific/intermediate subtask
- Fast to compute
- Helps to understand that system
- Not clear if really helpful unless correlation to real task is established

Suppose we build a model that models word similarity with word vectors. It would be very easy to test if it's working since we can test the similarities easily (specific subtask) .

##### **<u>Extrinsic</u>**

- Evaluation on a real task
- Can take a long time to compute accuracy
- Unclear if the subsystem is the problem or its interaction or other subsystems
- If replacing exactly one subsystem with another improves accuracy -> Winning

If we take the word similarity model above and insert into speech to text , then we try to evaluate it. It will be hard to know if our model was effective or not.

Maybe the model works perfectly but due to the way it is incorporated with the speech to text , It can't perform as well as it can.

------

The number of dimensions in the word vectors feeds into the Bias-Variance Trade off.

It was found that word vectors trained on Wikipedia (encyclopedias) find better relations between words due to the structure of Wikipedia that ties everything together . Compared to vectors trained on newspaper articles that only take one topic and talks about it 

------

### <u>**Polysemy**</u>

Words that have multiple meanings.

What was done was that for each common word , they made clusters to all the context words.

so for example the word jaguar had 5 clusters. They made 5 words called jaguar_1 , jaguar_2 ... , jaguar_5.

and treat each one separately.

#### **<u>Linear Algebraic Structure of Word Senses , with Applications to Polysemy</u>**

Different senses of a word reside in a linear superposition (weighted sum) in standard word embeddings like word2vec
$$
v_{\text{pike}} = \alpha_1 v_{\text{pike1}} +\alpha_2 v_{\text{pike2}} + \alpha_3 v_{\text{pike3}} 
$$
where $\alpha_1 = \frac{f_1}{f_1+f_2+f_3}$ for frequency f .

**Surprising result**:

​	Because of ideas from *sparse* coding , you can actually separate out the senses (providing they are relatively common)

