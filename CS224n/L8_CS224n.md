# <u>**L8_CS224n**</u>

## **<u>Machine Translation (MT)</u>**

It is the process of changing a sentence from the **source** language into the **target** language

****

### <u>**History of MT**</u>

#### <u>**Lookup Tables**</u>

In older days, MT was all about rule based systems which was basically a lookup table where source language and target language dictionaries were stored.

****

#### **<u>Statistical Machine Translation</u>**

- **Idea**: Learn a **probabilistic model** from **data**
- Assume translation is **French -> English**
- **Goal**: find the **best english sentence** $y$, given **french sentence** $x$ $argmax_yP(y|x)$

![](./Images/L8-images/StatMT1.png)

The task is divided into two models, so that each model can focus more on a specific task

****

#### **<u>How to learn translation model $P(x|y)$ ?</u>**

![](./Images/L8-images/StatMT2.png)

##### <u>Learning translation model $P(x|y)$ from the parallel corpus.</u>

We break it down further into $P(x,a|y)$ where $a$ is the **alignment**

**Alignment**: word-level correspondence between French sentence $x$ and English sentence $y$

****

##### <u>What is alignment ?</u>

Alignment is the **correspondence between particular words** in the translated sentence pair.

![](/home/hos/Studying/66DaysOfData/CS224N_Notes/Images/L8-images/Alignment1.png)

Notice that the word **Le** in French has no counterpart in English.

The word **Le** is called a **spurious word** because it has no counterpart in English.

****

###### <u>**Many-to-One Alignment**</u>

![](./Images/L8-images/Alignment2.png)

###### **<u>One-to-Many Alignment</u>**

![](./Images/L8-images/Alignment3.png)

The word **implemented** is called a **fertile word** because it maps to more than one word.

![](./Images/L8-images/Alignment4.png)

###### <u>**Many-to-Many Alignment**</u>

![](./Images/L8-images/Alignment5.png)

This is called **Phrase-level translation**.

****

##### **<u>Learning alignment for SMT</u>**

We learn $P(x,a|y)$ as a combination of many factors, including:

- Probability of particular words aligning (also depends on position in sentence)
- Probability of particular words having particular **fertility** (# of corresponding words)
- etc..

****

##### **<u>Decoding for SMT</u>**

###### **<u>How to compute $argmax_y P(x|y)P(y)$ ?</u>**

- Computing the probability of every possible $y$ and getting the max (**Too Expensive**)
- **Answer**: use a **heuristic search algorithm** to **search for the best translation**, discarding hypotheses that are too low-probability
- This process is called **decoding**

![](./Images/L8-images/DecodingSMT.png)

****

##### **<u>Statistical Machine Translation</u>**

SMT was a **huge research field**

The best systems were **extremely complex**

- Systems had many **separately-designed subcomponents** (we only discussed two)
- Lots of **feature engineering**
  - Need to design features to capture particular language phenomena
- Require compiling and maintaining **extra resources**
  - Like **tables of equivalent phrases**
- Lots of **human effort** to maintain
  - Repeated effort for each language **pair**

****

## <u>**Neural Machine Translation**</u>

**Neural Machine Translation (NMT)** is a way to do MT with a single neural network. The architecture used for this is called **sequence-to-sequence**(aka **seq2seq**) and it involves **two RNNs**.

The two RNNs are called **encoder** and **decoder RNNs**.

The decoder RNN is a **conditional RNN**. It is conditioned based on the last hidden state $h^{(T)}$ of the **encoder RNN** which is fed to the **decoder RNN** as the **hidden state** $h^{(0)}$.

Each language requires a separate **word embedding**.

![](./Images/L8-images/NMT_test.png)

Keep in mind that this is the test time behaviour.

- At test time, the output of the decoder RNN at time $t ,\ y^{(t)}$  is fed as an input for the next timestep (i.e. $x^{(t+1)} = y^{(t)}$)

****

## <u>Sequence-to-sequence is versatile !</u>

Seq2seq is useful for **more than just MT**.

Many NLP tasks can be phrased as seq2seq

- **Summarization** (long text -> short text)
- **Dialogue** (previous utterances -> next utterance)
- **Parsing** (input text -> output parse as sequence). This might not be the best option but an option nonetheless.
- **Code generation** (natural language -> Python code)

****

## <u>NMT Recap</u>

The **sequence-to-sequence** model is an example of a **Conditional Language Model**

- **Language Model** because the decoder is predicting the next word of the target sentence $y$

- **Conditional** because its predictions are condition on the source sentence $x$

- NMT directly calculates $P(y|x)$

  $P(y|x) = P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)......P(y_T|y_1,y_2....,y_{T-1},x)$

****

## <u>Training a Neural Machine Translation system</u>

![](./Images/L8-images/NMT_training.png)

**N.B.** unlike the test time, where the output in timestep $t$ is fed to the input in step $t+1$When training, the input to the **decoder RNN** is the correct sentence in the target language.

Sometimes, if the sentences don't have the same length. We pad the sentences with zeros to match the largest sentence in the corpus. But make sure not to use any hidden state that came from paddings.

We train the model end-to-end to make sure that the two RNNs work well together. as by doing that you are more sure that the two models work well together which results in **optimizing the model as a whole**.

There is research being done in training a general encoder for each language. (i.e. having a general English encoder and 2 general French and Arabic decoders. If that is achieved, this would allow us to mix and match. English Encoder -> Arabic decoder or Same English encoder -> French decoder and both would work)

****

## <u>Greedy decoding</u>

Since the decoder outputs a PMF of all the vocabulary indicating which word is more likely. Greedy decoding takes only the most likely path.

The problem is that there's no way to know which path is most likely unless you take all of them (which is not feasible $O(|V|^T)$)

****

## <u>Beam search decoding</u>

**Cora Idea**: On each step of the decoder, keep track of the **k most probable** partial translations which are called **hypotheses**.

- k is the **beam size** (in practice around **5 to 10**)

![](./Images/L8-images/BeamSearchScore.png)

### Beam Search Example

![](./Images/L8-images/BeamSearchExample.png)

Notice that if left that way the model is bound to favor **shorter translations**. So when a sentence has ended. It is divided by its length as a form of normalization.

****

### <u>Beam Search: Stopping criteria</u>

- In **greedy decoding**, usually we stop when the model produce the **<END> token**.

  i.e. <Start> he hit me with a pie <END>

- In **beam search decoding**, different hypotheses may produce <END> tokens on **different timesteps**

  - When a hypothesis produces <END>, that hypothesis is **complete**
  - **Place it aside** and continue exploring other hypotheses via beam search

- Usually we continue beam search until

  - We reach **timestep $T$** where $T$ is some **predefined cutoff**, or
  - We have at least **$n$ completed hypothesis** where $n$ is some **predefined cutoff**

****

## <u>Comparison between NMT & SMT</u>

### <u>Advantages of NMT</u>

compared to SMT, NMT has many **advantages**

- Better **performance**
  - More **fluent**
  - Better use of **context**
  - Better use of **phrase similarities**
- A **single** neural network to be optimized **end-to-end**
  - No subcomponents to be **individually optimized**
- Requires much **less human engineering effort**
  - No feature engineering
  - Same method for all **language pairs** (same architecture but different data)

****

### <u>**Disadvantages** compared to SMT</u>

- NMT is **less interpretable**
  - **Hard to debug** (while SMT is also not very interpretable, it is atleast kind of interpretable (more than NMT))
- NMT is **difficult to control**
  - You can't easily **specify rules or guidelines** for translation
  - Safety concerns (can't expect what the model will say)

****

##  <u>Evaluating MT</u>

### <u>**BLEU**  (Bilingual Evaluation Understudy)</u>

- BLUE compares the **machine written translation** to one or several **human-written translation(s)**, and computes a **similarity score** based on:

  - **n-gram precision** (usually for 1,2,3 and 4-grams)

  - Plus a **penalty** for too-**short** system translations

    If that is not added, a system could output the information that it is only 100% certain about and leave other information out of the translation.

- BLEU is useful but **imperfect**

  - There are many **valid** ways to translate a sentence

    This may cause a **good** translation to get a **poor** BLEU score as it has low n-gram overlap with the human translation.

****

## <u>MT Shortcomings</u>

- **Out-of-vocabulary** words.
- **Domain mismatch** between train and test data (training on wiki then test on twitter (formal vs informal)).
- Maintaining **context** over longer text.
- **Low-resource** language pairs.
- Using **common sense** is hard.
  - i.e. (paper jam translated into ورق مربى)
- Picking up **biases** in training data.
- **Uninterpretable** systems do **strange things**.

****

## <u>NMT Research</u>

- NMT research still continues to now and **many, many improvements** were found to the **vanilla seq2seq** NMT presented above
- One improvement is so integral that it is now the **new vanilla**, It is **ATTENTION**

****

## <u>**Attention**</u>

### <u>**Motivation**</u>

![](./Images/L8-images/Attention1.png)

The problem is that the last hidden state of the encoder needs to encode all the data from the sentence correctly, So this state become an **Information bottleneck**.

****

### **<u>Solution</u>**

On each step of the decoder, use **direct connection to the encoder** to **focus on a particular part** of the source sequence.

![](./Images/L8-images/Attention2.png)

#### **<u>Steps</u>**

1. Take the **dot product** of **first hidden state** the **decoder** and **each hidden state** from the **encoder**, The output of each dot product is called **Attention score**

2. Take the **softmax** of the attention scores, and that outputs an **Attention distribution**.

3. ![](./Images/L8-images/Attention3.png)

   We then take a weighted average of the **encoder hidden states** with the **attention distribution** being the weight $\text{Attention Output} = \sum_{t=1}^T \text{Attention Distribution}_t \cdot   h^{(t)}$ . The **Attention output** mostly contains Info from the **hidden state** that received **high attention**.

4. ![](./Images/L8-images/Attention4.png)

   Concatenate the **attention output** with **decoder hidden state**, then use to compute $\tilde y_1$ as before (use **attention output <u>and</u> decoder hidden state** instead of only the hidden state) 

5. ![](./Images/L8-images/Attention5.png)

   Repeat these steps for the next words. 
   
   Sometimes, we take the **attention output** from the previous step and also feed it into the **decoder**(along with the usual decoder input). This isn't always done

****

## <u>Attention Equations</u>

- For **encoder hidden states**, $h_1,.....,h_N \ \epsilon \ \R^{h}$
- On timestep $t$, we have **decoder hidden state** $s_t \ \epsilon \ \R^{h}$ 

- Attention steps are

  1. Computing the **attention scores** $e^t = [s_t^T h_1, ......, s_t^Th_N] \ \epsilon \ \R^{N}$

  2. Taking the softmax to get **attention distribution** $\alpha$:

     ​						$\alpha^t = \text{softmax(}e^t) \  \epsilon \ \R^N$

  3. Using attention distribution to take weighted sum of values

     ​				$a_t = \sum_{i=1}^N \alpha^t_i h_i \ \epsilon \ \R^{h}$

     Thus obtaining the **attention output** $a$ (sometimes called the **context vector**)

  4. Finally, we concatenate the **attention output** with the **decoder hidden state** $s_t$ and proceed as in the non-attention seq2seq model

     ​											$[a_t;s_t] \ \epsilon \ \R^{2h}$

While steps 2 and 3 always happen, there are **multiple ways** to compute **attention scores** in step 1.



****

## <u>Why attention helps?</u>

- Attention **improves NMT performance** as it allows the decoder to **focus on certain parts of the source**
- Attention solves the **bottleneck problem**
- Attention helps with **vanishing gradient problem** by adding **shortcut connections**
- Attention provides **some interpretability **.
  - As **attention distribution** will show us what the decoder was focusing on.
  - ![](./Images/L8-images/Attention6.png)
  - This is a form of **alignment for free** (without specifically telling the model to create alignment (like in SMT), It is automatically created for you with way less effort.)
  - The network just learned alignment by itself

****

## **<u>Attention is a general DL technique</u>**

Watch yt video waterloo

****

## <u>**Attention Variants**</u>

For **values**, $h_1,.....,h_N \ \epsilon \ \R^{d_1}$ and a **query** $s \ \epsilon \ \R^{d_2}$

- Attention steps are

  1. Computing the **attention scores** $e \ \epsilon \ \R^{N}$

  2. Taking the softmax to get **attention distribution** $\alpha$:

     ​						$\alpha = \text{softmax(}e) \  \epsilon \ \R^N$

  3. Using attention distribution to take weighted sum of values

     ​				$a = \sum_{i=1}^N \alpha_i h_i \ \epsilon \ \R^{d_1}$

     Thus obtaining the **attention output** $a$ (sometimes called the **context vector**)

While steps 2 and 3 always happen, there are **multiple ways** to compute **attention scores** in step 1.

****

Keep in mind that the attention score is always a **scalar**. 

### <u>Basic dot-product attention</u>

​															$e_i = s^T h_i \ \epsilon \ \R$

This assumes that $d_1 = d_2$

### <u>Multiplicative attention</u>

​														$e_i = s^T \ W \ h_i \ \epsilon \ \R$

Where $W  \ \epsilon \ \R^{d_2 \times d_1}$ is a learnable **weight matrix**

### <u>**Additive Attention**</u>

​														$e_i = v^T \tanh( W_1  h_i + W_2s) \ \epsilon \ \R$

Where  $W_1  \ \epsilon \ \R^{d_3 \times d_1},  W_2  \ \epsilon \ \R^{d_3 \times d_2}$ are **weight matrices** and $v  \ \epsilon \ \R^{d_3}$$ is a **weight vector**

$d_3$ (the attention dimensionality) is a hyperparameter.

​	

